[2019-10-02 18:31:22,102] {taskinstance.py:620} INFO - Dependencies all met for <TaskInstance: capstone_project_dag.load_staging_database_tables 2019-10-02T23:15:28.523483+00:00 [queued]>
[2019-10-02 18:31:22,111] {taskinstance.py:620} INFO - Dependencies all met for <TaskInstance: capstone_project_dag.load_staging_database_tables 2019-10-02T23:15:28.523483+00:00 [queued]>
[2019-10-02 18:31:22,111] {taskinstance.py:838} INFO - 
--------------------------------------------------------------------------------
[2019-10-02 18:31:22,111] {taskinstance.py:839} INFO - Starting attempt 4 of 4
[2019-10-02 18:31:22,111] {taskinstance.py:840} INFO - 
--------------------------------------------------------------------------------
[2019-10-02 18:31:22,130] {taskinstance.py:859} INFO - Executing <Task(PostgresOperator): load_staging_database_tables> on 2019-10-02T23:15:28.523483+00:00
[2019-10-02 18:31:22,130] {base_task_runner.py:133} INFO - Running: ['airflow', 'run', 'capstone_project_dag', 'load_staging_database_tables', '2019-10-02T23:15:28.523483+00:00', '--job_id', '179', '--pool', 'default_pool', '--raw', '-sd', 'DAGS_FOLDER/project_dag.py', '--cfg_path', '/var/folders/dr/9yvjlnm11xv25k2gknf2n3z00000gn/T/tmpmj6fuwqv']
[2019-10-02 18:31:22,747] {base_task_runner.py:115} INFO - Job 179: Subtask load_staging_database_tables /usr/local/lib/python3.7/site-packages/airflow/configuration.py:627: DeprecationWarning: You have two airflow.cfg files: /Users/victorialemay/airflow/airflow.cfg and /Users/victorialemay/Courses/Udacity/Data Engineering Nanodegree/capstone/project-archive/airflow/airflow.cfg. Airflow used to look at ~/airflow/airflow.cfg, even when AIRFLOW_HOME was set to a different value. Airflow will now only read /Users/victorialemay/Courses/Udacity/Data Engineering Nanodegree/capstone/project-archive/airflow/airflow.cfg, and you should remove the other file
[2019-10-02 18:31:22,748] {base_task_runner.py:115} INFO - Job 179: Subtask load_staging_database_tables   category=DeprecationWarning,
[2019-10-02 18:31:22,851] {base_task_runner.py:115} INFO - Job 179: Subtask load_staging_database_tables [2019-10-02 18:31:22,850] {settings.py:213} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=30308
[2019-10-02 18:31:23,158] {base_task_runner.py:115} INFO - Job 179: Subtask load_staging_database_tables [2019-10-02 18:31:23,158] {__init__.py:51} INFO - Using executor LocalExecutor
[2019-10-02 18:31:23,622] {base_task_runner.py:115} INFO - Job 179: Subtask load_staging_database_tables [2019-10-02 18:31:23,622] {dagbag.py:90} INFO - Filling up the DagBag from /Users/victorialemay/Courses/Udacity/Data Engineering Nanodegree/capstone/project-archive/airflow/dags/project_dag.py
[2019-10-02 18:31:23,697] {base_task_runner.py:115} INFO - Job 179: Subtask load_staging_database_tables [2019-10-02 18:31:23,696] {cli.py:516} INFO - Running <TaskInstance: capstone_project_dag.load_staging_database_tables 2019-10-02T23:15:28.523483+00:00 [running]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[2019-10-02 18:31:23,747] {postgres_operator.py:62} INFO - Executing: CREATE TABLE IF NOT EXISTS public.i94_immigration_staging (
    id int4,
    cicid int4,
    i94yr int4,
    i94mon int4,
    i94cit int4,
    i94res int4,
    i94port int4,
    arrdate int4,
    i94mode int4,
    i94addr varchar(12),
    depdate int4,
    i94bir int4,
    i94visa int4,
    "count" int4,
    dtadfile varchar(16),
    visapost varchar(16),
    occup varchar(256),
    entdepa char(1),
    entdepd char(1),
    entdepu char(1),
    matflag char(1),
    biryear int4,
    dtaddto varchar(16),
    gender char(1),
    insnum int4,
    airline varchar(32),
    admnum int4,
    fltno varchar(12),
    visa_type varchar(12)
);

CREATE TABLE IF NOT EXISTS public.land_temp_staging (
    dt varchar(12),
    AverageTemperature numeric(18,0),
    AverageTemperatureUncertainty numeric(18,0),
    City varchar(256),
    Country varchar(256),
    Latitude varchar(64),
    Longitude varchar(64)
);

CREATE TABLE IF NOT EXISTS public.demographic_staging (
    City varchar(256),
    State varchar(128),
    MedianAge numeric(18,0),
    MalePopulation int4,
    FemalePopulation int4,
    TotalPopulation int4,
    NumberOfVeterans int4,
    ForeignBorn int4,
    AverageHouseholdSize numeric(18,0),
    StateCode char(2),
    Race varchar(128),
    Count int4
);

CREATE TABLE IF NOT EXISTS public.port_code_staging (
    name varchar(512),
    state_code varchar(2),
    country_name varchar(64),
    i94_port_code varchar(3)
);
[2019-10-02 18:31:23,762] {logging_mixin.py:95} INFO - [[34m2019-10-02 18:31:23,762[0m] {[34mbase_hook.py:[0m84} INFO[0m - Using connection to: [1mid: redshift. Host: redshift-cluster-1.c4i0rfzboubh.us-west-2.redshift.amazonaws.com, Port: 5439, Schema: dev, Login: awsuser, Password: XXXXXXXX, extra: {}[0m[0m
[2019-10-02 18:31:23,764] {taskinstance.py:1051} ERROR - could not translate host name "redshift-cluster-1.c4i0rfzboubh.us-west-2.redshift.amazonaws.com" to address: nodename nor servname provided, or not known
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 926, in _run_raw_task
    result = task_copy.execute(context=context)
  File "/usr/local/lib/python3.7/site-packages/airflow/operators/postgres_operator.py", line 65, in execute
    self.hook.run(self.sql, self.autocommit, parameters=self.parameters)
  File "/usr/local/lib/python3.7/site-packages/airflow/hooks/dbapi_hook.py", line 159, in run
    with closing(self.get_conn()) as conn:
  File "/usr/local/lib/python3.7/site-packages/airflow/hooks/postgres_hook.py", line 75, in get_conn
    self.conn = psycopg2.connect(**conn_args)
  File "/usr/local/lib/python3.7/site-packages/psycopg2/__init__.py", line 126, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "redshift-cluster-1.c4i0rfzboubh.us-west-2.redshift.amazonaws.com" to address: nodename nor servname provided, or not known

[2019-10-02 18:31:23,768] {taskinstance.py:1080} INFO - All retries failed; marking task as FAILED
[2019-10-02 18:31:23,799] {base_task_runner.py:115} INFO - Job 179: Subtask load_staging_database_tables Traceback (most recent call last):
[2019-10-02 18:31:23,799] {base_task_runner.py:115} INFO - Job 179: Subtask load_staging_database_tables   File "/usr/local/bin/airflow", line 32, in <module>
[2019-10-02 18:31:23,799] {base_task_runner.py:115} INFO - Job 179: Subtask load_staging_database_tables     args.func(args)
[2019-10-02 18:31:23,799] {base_task_runner.py:115} INFO - Job 179: Subtask load_staging_database_tables   File "/usr/local/lib/python3.7/site-packages/airflow/utils/cli.py", line 74, in wrapper
[2019-10-02 18:31:23,799] {base_task_runner.py:115} INFO - Job 179: Subtask load_staging_database_tables     return f(*args, **kwargs)
[2019-10-02 18:31:23,799] {base_task_runner.py:115} INFO - Job 179: Subtask load_staging_database_tables   File "/usr/local/lib/python3.7/site-packages/airflow/bin/cli.py", line 522, in run
[2019-10-02 18:31:23,799] {base_task_runner.py:115} INFO - Job 179: Subtask load_staging_database_tables     _run(args, dag, ti)
[2019-10-02 18:31:23,799] {base_task_runner.py:115} INFO - Job 179: Subtask load_staging_database_tables   File "/usr/local/lib/python3.7/site-packages/airflow/bin/cli.py", line 440, in _run
[2019-10-02 18:31:23,800] {base_task_runner.py:115} INFO - Job 179: Subtask load_staging_database_tables     pool=args.pool,
[2019-10-02 18:31:23,800] {base_task_runner.py:115} INFO - Job 179: Subtask load_staging_database_tables   File "/usr/local/lib/python3.7/site-packages/airflow/utils/db.py", line 74, in wrapper
[2019-10-02 18:31:23,800] {base_task_runner.py:115} INFO - Job 179: Subtask load_staging_database_tables     return func(*args, **kwargs)
[2019-10-02 18:31:23,800] {base_task_runner.py:115} INFO - Job 179: Subtask load_staging_database_tables   File "/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 926, in _run_raw_task
[2019-10-02 18:31:23,800] {base_task_runner.py:115} INFO - Job 179: Subtask load_staging_database_tables     result = task_copy.execute(context=context)
[2019-10-02 18:31:23,800] {base_task_runner.py:115} INFO - Job 179: Subtask load_staging_database_tables   File "/usr/local/lib/python3.7/site-packages/airflow/operators/postgres_operator.py", line 65, in execute
[2019-10-02 18:31:23,800] {base_task_runner.py:115} INFO - Job 179: Subtask load_staging_database_tables     self.hook.run(self.sql, self.autocommit, parameters=self.parameters)
[2019-10-02 18:31:23,800] {base_task_runner.py:115} INFO - Job 179: Subtask load_staging_database_tables   File "/usr/local/lib/python3.7/site-packages/airflow/hooks/dbapi_hook.py", line 159, in run
[2019-10-02 18:31:23,801] {base_task_runner.py:115} INFO - Job 179: Subtask load_staging_database_tables     with closing(self.get_conn()) as conn:
[2019-10-02 18:31:23,801] {base_task_runner.py:115} INFO - Job 179: Subtask load_staging_database_tables   File "/usr/local/lib/python3.7/site-packages/airflow/hooks/postgres_hook.py", line 75, in get_conn
[2019-10-02 18:31:23,801] {base_task_runner.py:115} INFO - Job 179: Subtask load_staging_database_tables     self.conn = psycopg2.connect(**conn_args)
[2019-10-02 18:31:23,801] {base_task_runner.py:115} INFO - Job 179: Subtask load_staging_database_tables   File "/usr/local/lib/python3.7/site-packages/psycopg2/__init__.py", line 126, in connect
[2019-10-02 18:31:23,801] {base_task_runner.py:115} INFO - Job 179: Subtask load_staging_database_tables     conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
[2019-10-02 18:31:23,801] {base_task_runner.py:115} INFO - Job 179: Subtask load_staging_database_tables psycopg2.OperationalError: could not translate host name "redshift-cluster-1.c4i0rfzboubh.us-west-2.redshift.amazonaws.com" to address: nodename nor servname provided, or not known
[2019-10-02 18:31:23,801] {base_task_runner.py:115} INFO - Job 179: Subtask load_staging_database_tables 
[2019-10-02 18:31:27,084] {logging_mixin.py:95} INFO - [[34m2019-10-02 18:31:27,084[0m] {[34mlocal_task_job.py:[0m105} INFO[0m - Task exited with return code 1[0m
