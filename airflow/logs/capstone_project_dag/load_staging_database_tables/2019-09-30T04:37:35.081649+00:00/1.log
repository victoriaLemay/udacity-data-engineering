[2019-09-29 23:37:48,514] {taskinstance.py:620} INFO - Dependencies all met for <TaskInstance: capstone_project_dag.load_staging_database_tables 2019-09-30T04:37:35.081649+00:00 [queued]>
[2019-09-29 23:37:48,522] {taskinstance.py:620} INFO - Dependencies all met for <TaskInstance: capstone_project_dag.load_staging_database_tables 2019-09-30T04:37:35.081649+00:00 [queued]>
[2019-09-29 23:37:48,523] {taskinstance.py:838} INFO - 
--------------------------------------------------------------------------------
[2019-09-29 23:37:48,523] {taskinstance.py:839} INFO - Starting attempt 1 of 4
[2019-09-29 23:37:48,523] {taskinstance.py:840} INFO - 
--------------------------------------------------------------------------------
[2019-09-29 23:37:48,541] {taskinstance.py:859} INFO - Executing <Task(PostgresOperator): load_staging_database_tables> on 2019-09-30T04:37:35.081649+00:00
[2019-09-29 23:37:48,541] {base_task_runner.py:133} INFO - Running: ['airflow', 'run', 'capstone_project_dag', 'load_staging_database_tables', '2019-09-30T04:37:35.081649+00:00', '--job_id', '59', '--pool', 'default_pool', '--raw', '-sd', 'DAGS_FOLDER/project_dag.py', '--cfg_path', '/var/folders/dr/9yvjlnm11xv25k2gknf2n3z00000gn/T/tmpvs0q3nua']
[2019-09-29 23:37:49,001] {base_task_runner.py:115} INFO - Job 59: Subtask load_staging_database_tables /usr/local/lib/python3.7/site-packages/airflow/configuration.py:627: DeprecationWarning: You have two airflow.cfg files: /Users/victorialemay/airflow/airflow.cfg and /Users/victorialemay/Courses/Udacity/Data Engineering Nanodegree/capstone/project-archive/airflow/airflow.cfg. Airflow used to look at ~/airflow/airflow.cfg, even when AIRFLOW_HOME was set to a different value. Airflow will now only read /Users/victorialemay/Courses/Udacity/Data Engineering Nanodegree/capstone/project-archive/airflow/airflow.cfg, and you should remove the other file
[2019-09-29 23:37:49,001] {base_task_runner.py:115} INFO - Job 59: Subtask load_staging_database_tables   category=DeprecationWarning,
[2019-09-29 23:37:49,088] {base_task_runner.py:115} INFO - Job 59: Subtask load_staging_database_tables [2019-09-29 23:37:49,088] {settings.py:213} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=56434
[2019-09-29 23:37:49,221] {base_task_runner.py:115} INFO - Job 59: Subtask load_staging_database_tables [2019-09-29 23:37:49,221] {__init__.py:51} INFO - Using executor LocalExecutor
[2019-09-29 23:37:49,599] {base_task_runner.py:115} INFO - Job 59: Subtask load_staging_database_tables [2019-09-29 23:37:49,599] {dagbag.py:90} INFO - Filling up the DagBag from /Users/victorialemay/Courses/Udacity/Data Engineering Nanodegree/capstone/project-archive/airflow/dags/project_dag.py
[2019-09-29 23:37:49,635] {base_task_runner.py:115} INFO - Job 59: Subtask load_staging_database_tables [2019-09-29 23:37:49,634] {cli.py:516} INFO - Running <TaskInstance: capstone_project_dag.load_staging_database_tables 2019-09-30T04:37:35.081649+00:00 [running]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[2019-09-29 23:37:49,661] {postgres_operator.py:62} INFO - Executing: CREATE TABLE IF NOT EXISTS public.i94_immigration_staging (
    id int4,
    cicid int4,
    i94yr int4,
    i94mon int4,
    i94cit int4,
    i94res int4,
    i94port int4,
    arrdate int4,
    i94mode int4,
    i94addr varchar(12),
    depdate int4,
    i94bir int4,
    i94visa int4,
    "count" int4,
    dtadfile varchar(16),
    visapost varchar(16),
    occup varchar(256),
    entdepa char(1),
    entdepd char(1),
    entdepu char(1),
    matflag char(1),
    biryear int4,
    dtaddto varchar(16),
    gender char(1),
    insnum int4,
    airline varchar(32),
    admnum int4,
    fltno varchar(12),
    visa_type varchar(12)
);

CREATE TABLE IF NOT EXISTS public.land_temp_staging (
    dt varchar(12),
    AverageTemperature numeric(18,0),
    AverageTemperatureUncertainty numeric(18,0),
    City varchar(256),
    Country varchar(256),
    Latitude varchar(64),
    Longitude varchar(64)
);

CREATE TABLE IF NOT EXISTS public.demographic_staging (
    City varchar(256),
    State varchar(128),
    MedianAge numeric(18,0),
    MalePopulation int4,
    FemalePopulation int4,
    TotalPopulation int4,
    NumberOfVeterans int4,
    ForeignBorn int4,
    AverageHouseholdSize numeric(18,0),
    StateCode char(2),
    Race varchar(128),
    Count int4
);
[2019-09-29 23:37:49,684] {taskinstance.py:1051} ERROR - The conn_id `redshift` isn't defined
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 926, in _run_raw_task
    result = task_copy.execute(context=context)
  File "/usr/local/lib/python3.7/site-packages/airflow/operators/postgres_operator.py", line 65, in execute
    self.hook.run(self.sql, self.autocommit, parameters=self.parameters)
  File "/usr/local/lib/python3.7/site-packages/airflow/hooks/dbapi_hook.py", line 159, in run
    with closing(self.get_conn()) as conn:
  File "/usr/local/lib/python3.7/site-packages/airflow/hooks/postgres_hook.py", line 56, in get_conn
    conn = self.get_connection(self.postgres_conn_id)
  File "/usr/local/lib/python3.7/site-packages/airflow/hooks/base_hook.py", line 81, in get_connection
    conn = random.choice(list(cls.get_connections(conn_id)))
  File "/usr/local/lib/python3.7/site-packages/airflow/hooks/base_hook.py", line 76, in get_connections
    conns = cls._get_connections_from_db(conn_id)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/db.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/airflow/hooks/base_hook.py", line 59, in _get_connections_from_db
    "The conn_id `{0}` isn't defined".format(conn_id))
airflow.exceptions.AirflowException: The conn_id `redshift` isn't defined
[2019-09-29 23:37:49,689] {taskinstance.py:1074} INFO - Marking task as UP_FOR_RETRY
[2019-09-29 23:37:49,711] {base_task_runner.py:115} INFO - Job 59: Subtask load_staging_database_tables Traceback (most recent call last):
[2019-09-29 23:37:49,711] {base_task_runner.py:115} INFO - Job 59: Subtask load_staging_database_tables   File "/usr/local/bin/airflow", line 32, in <module>
[2019-09-29 23:37:49,711] {base_task_runner.py:115} INFO - Job 59: Subtask load_staging_database_tables     args.func(args)
[2019-09-29 23:37:49,712] {base_task_runner.py:115} INFO - Job 59: Subtask load_staging_database_tables   File "/usr/local/lib/python3.7/site-packages/airflow/utils/cli.py", line 74, in wrapper
[2019-09-29 23:37:49,712] {base_task_runner.py:115} INFO - Job 59: Subtask load_staging_database_tables     return f(*args, **kwargs)
[2019-09-29 23:37:49,712] {base_task_runner.py:115} INFO - Job 59: Subtask load_staging_database_tables   File "/usr/local/lib/python3.7/site-packages/airflow/bin/cli.py", line 522, in run
[2019-09-29 23:37:49,712] {base_task_runner.py:115} INFO - Job 59: Subtask load_staging_database_tables     _run(args, dag, ti)
[2019-09-29 23:37:49,712] {base_task_runner.py:115} INFO - Job 59: Subtask load_staging_database_tables   File "/usr/local/lib/python3.7/site-packages/airflow/bin/cli.py", line 440, in _run
[2019-09-29 23:37:49,712] {base_task_runner.py:115} INFO - Job 59: Subtask load_staging_database_tables     pool=args.pool,
[2019-09-29 23:37:49,712] {base_task_runner.py:115} INFO - Job 59: Subtask load_staging_database_tables   File "/usr/local/lib/python3.7/site-packages/airflow/utils/db.py", line 74, in wrapper
[2019-09-29 23:37:49,712] {base_task_runner.py:115} INFO - Job 59: Subtask load_staging_database_tables     return func(*args, **kwargs)
[2019-09-29 23:37:49,712] {base_task_runner.py:115} INFO - Job 59: Subtask load_staging_database_tables   File "/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 926, in _run_raw_task
[2019-09-29 23:37:49,713] {base_task_runner.py:115} INFO - Job 59: Subtask load_staging_database_tables     result = task_copy.execute(context=context)
[2019-09-29 23:37:49,713] {base_task_runner.py:115} INFO - Job 59: Subtask load_staging_database_tables   File "/usr/local/lib/python3.7/site-packages/airflow/operators/postgres_operator.py", line 65, in execute
[2019-09-29 23:37:49,713] {base_task_runner.py:115} INFO - Job 59: Subtask load_staging_database_tables     self.hook.run(self.sql, self.autocommit, parameters=self.parameters)
[2019-09-29 23:37:49,713] {base_task_runner.py:115} INFO - Job 59: Subtask load_staging_database_tables   File "/usr/local/lib/python3.7/site-packages/airflow/hooks/dbapi_hook.py", line 159, in run
[2019-09-29 23:37:49,713] {base_task_runner.py:115} INFO - Job 59: Subtask load_staging_database_tables     with closing(self.get_conn()) as conn:
[2019-09-29 23:37:49,713] {base_task_runner.py:115} INFO - Job 59: Subtask load_staging_database_tables   File "/usr/local/lib/python3.7/site-packages/airflow/hooks/postgres_hook.py", line 56, in get_conn
[2019-09-29 23:37:49,713] {base_task_runner.py:115} INFO - Job 59: Subtask load_staging_database_tables     conn = self.get_connection(self.postgres_conn_id)
[2019-09-29 23:37:49,713] {base_task_runner.py:115} INFO - Job 59: Subtask load_staging_database_tables   File "/usr/local/lib/python3.7/site-packages/airflow/hooks/base_hook.py", line 81, in get_connection
[2019-09-29 23:37:49,713] {base_task_runner.py:115} INFO - Job 59: Subtask load_staging_database_tables     conn = random.choice(list(cls.get_connections(conn_id)))
[2019-09-29 23:37:49,713] {base_task_runner.py:115} INFO - Job 59: Subtask load_staging_database_tables   File "/usr/local/lib/python3.7/site-packages/airflow/hooks/base_hook.py", line 76, in get_connections
[2019-09-29 23:37:49,713] {base_task_runner.py:115} INFO - Job 59: Subtask load_staging_database_tables     conns = cls._get_connections_from_db(conn_id)
[2019-09-29 23:37:49,713] {base_task_runner.py:115} INFO - Job 59: Subtask load_staging_database_tables   File "/usr/local/lib/python3.7/site-packages/airflow/utils/db.py", line 74, in wrapper
[2019-09-29 23:37:49,714] {base_task_runner.py:115} INFO - Job 59: Subtask load_staging_database_tables     return func(*args, **kwargs)
[2019-09-29 23:37:49,714] {base_task_runner.py:115} INFO - Job 59: Subtask load_staging_database_tables   File "/usr/local/lib/python3.7/site-packages/airflow/hooks/base_hook.py", line 59, in _get_connections_from_db
[2019-09-29 23:37:49,714] {base_task_runner.py:115} INFO - Job 59: Subtask load_staging_database_tables     "The conn_id `{0}` isn't defined".format(conn_id))
[2019-09-29 23:37:49,714] {base_task_runner.py:115} INFO - Job 59: Subtask load_staging_database_tables airflow.exceptions.AirflowException: The conn_id `redshift` isn't defined
[2019-09-29 23:37:53,501] {logging_mixin.py:95} INFO - [[34m2019-09-29 23:37:53,500[0m] {[34mlocal_task_job.py:[0m105} INFO[0m - Task exited with return code 1[0m
