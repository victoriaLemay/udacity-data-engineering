[2019-10-02 19:56:40,480] {taskinstance.py:620} INFO - Dependencies all met for <TaskInstance: capstone_project_dag.load_final_database_tables 2019-10-03T00:39:44.525076+00:00 [queued]>
[2019-10-02 19:56:40,488] {taskinstance.py:620} INFO - Dependencies all met for <TaskInstance: capstone_project_dag.load_final_database_tables 2019-10-03T00:39:44.525076+00:00 [queued]>
[2019-10-02 19:56:40,488] {taskinstance.py:838} INFO - 
--------------------------------------------------------------------------------
[2019-10-02 19:56:40,488] {taskinstance.py:839} INFO - Starting attempt 1 of 4
[2019-10-02 19:56:40,488] {taskinstance.py:840} INFO - 
--------------------------------------------------------------------------------
[2019-10-02 19:56:40,508] {taskinstance.py:859} INFO - Executing <Task(PostgresOperator): load_final_database_tables> on 2019-10-03T00:39:44.525076+00:00
[2019-10-02 19:56:40,508] {base_task_runner.py:133} INFO - Running: ['airflow', 'run', 'capstone_project_dag', 'load_final_database_tables', '2019-10-03T00:39:44.525076+00:00', '--job_id', '189', '--pool', 'default_pool', '--raw', '-sd', 'DAGS_FOLDER/project_dag.py', '--cfg_path', '/var/folders/dr/9yvjlnm11xv25k2gknf2n3z00000gn/T/tmpf9gsq2f1']
[2019-10-02 19:56:41,001] {base_task_runner.py:115} INFO - Job 189: Subtask load_final_database_tables /usr/local/lib/python3.7/site-packages/airflow/configuration.py:627: DeprecationWarning: You have two airflow.cfg files: /Users/victorialemay/airflow/airflow.cfg and /Users/victorialemay/Courses/Udacity/Data Engineering Nanodegree/capstone/project-archive/airflow/airflow.cfg. Airflow used to look at ~/airflow/airflow.cfg, even when AIRFLOW_HOME was set to a different value. Airflow will now only read /Users/victorialemay/Courses/Udacity/Data Engineering Nanodegree/capstone/project-archive/airflow/airflow.cfg, and you should remove the other file
[2019-10-02 19:56:41,001] {base_task_runner.py:115} INFO - Job 189: Subtask load_final_database_tables   category=DeprecationWarning,
[2019-10-02 19:56:41,095] {base_task_runner.py:115} INFO - Job 189: Subtask load_final_database_tables [2019-10-02 19:56:41,095] {settings.py:213} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=37741
[2019-10-02 19:56:41,324] {base_task_runner.py:115} INFO - Job 189: Subtask load_final_database_tables [2019-10-02 19:56:41,323] {__init__.py:51} INFO - Using executor LocalExecutor
[2019-10-02 19:56:41,779] {base_task_runner.py:115} INFO - Job 189: Subtask load_final_database_tables [2019-10-02 19:56:41,778] {dagbag.py:90} INFO - Filling up the DagBag from /Users/victorialemay/Courses/Udacity/Data Engineering Nanodegree/capstone/project-archive/airflow/dags/project_dag.py
[2019-10-02 19:56:41,823] {base_task_runner.py:115} INFO - Job 189: Subtask load_final_database_tables [2019-10-02 19:56:41,823] {cli.py:516} INFO - Running <TaskInstance: capstone_project_dag.load_final_database_tables 2019-10-03T00:39:44.525076+00:00 [running]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[2019-10-02 19:56:41,859] {postgres_operator.py:62} INFO - Executing: CREATE TABLE IF NOT EXISTS public.i94_immigration (
    id int4 NOT NULL,
    year int4,
    month int4,
    dest_country_code_id int4,
    res_country_code_id int4,
    port_id int4,
    travel_mode_id int4,
    state_code_id int4,
    arrival_date timestamp NOT NULL,
    departure_date timestamp NOT NULL,
    respondent_age int4,
    visa_type_code_id int4,
    visa_post_code_id int4,
    occupation varchar(256),
    birth_year int4,
    admitted_until_date timestamp NOT NULL,
    gender char(1),
    visa_type varchar(12),
    CONSTRAINT i94_immigration_pkey PRIMARY KEY (id)
);

CREATE TABLE IF NOT EXISTS public.country_codes (
    id int4 NOT NULL,
    name varchar(256),
    i94_code varchar(256)
    CONSTRAINT country_codes_pkey PRIMARY KEY (id)
);

CREATE TABLE IF NOT EXISTS public.travel_modes (
    id int4 NOT NULL,
    name varchar(256),
    travel_mode_code varchar(256)
    CONSTRAINT travel_modes_pkey PRIMARY KEY (id)
);

CREATE TABLE IF NOT EXISTS public.port_codes (
    id int4 NOT NULL,
    city varchar(256),
    state_id int4,
    i94_port_code varchar(3),
    CONSTRAINT city_state_codes_pkey PRIMARY KEY (id)
);

CREATE TABLE IF NOT EXISTS public.us_states (
    id int4 NOT NULL,
    name varchar(256),
    state_code varchar(2),
    CONSTRAINT us_states_pkey PRIMARY KEY (id)
);

CREATE TABLE IF NOT EXISTS public.visa_types (
    id int4 NOT NULL,
    name varchar(256),
    visa_type_code varchar(256)
    CONSTRAINT visa_types_pkey PRIMARY KEY (id)
);

CREATE TABLE IF NOT EXISTS public.land_temperatures (
    id int4 NOT NULL,
    date timestamp NOT NULL,
    avg_temp numeric(18,0),
    avg_temp_uncertainty numeric(18,0),
    city varchar(256),
    country_code_id int4,
    latitude numeric(18,0),
    longitude numeric(18,0),
    CONSTRAINT land_temperatures_pkey PRIMARY KEY (id)
);

CREATE TABLE IF NOT EXISTS public.us_city_demographics (
    id int4 NOT NULL,
    port_code_id int4,
    median_age numeric(18,0),
    male_pop int4,
    female_pop int4,
    total_pop int4,
    num_vets int4,
    foreign_pop int4,
    avg_household_size numeric(18,0),
    race varchar(256),
    race_pop int4,
    CONSTRAINT us_city_demographics_pkey PRIMARY KEY (id)
);
[2019-10-02 19:56:41,874] {logging_mixin.py:95} INFO - [[34m2019-10-02 19:56:41,874[0m] {[34mbase_hook.py:[0m84} INFO[0m - Using connection to: [1mid: redshift. Host: redshift-cluster-1.c4i0rfzboubh.us-west-2.redshift.amazonaws.com, Port: 5439, Schema: dev, Login: awsuser, Password: XXXXXXXX, extra: {}[0m[0m
[2019-10-02 19:56:41,946] {taskinstance.py:1051} ERROR - could not translate host name "redshift-cluster-1.c4i0rfzboubh.us-west-2.redshift.amazonaws.com" to address: nodename nor servname provided, or not known
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 926, in _run_raw_task
    result = task_copy.execute(context=context)
  File "/usr/local/lib/python3.7/site-packages/airflow/operators/postgres_operator.py", line 65, in execute
    self.hook.run(self.sql, self.autocommit, parameters=self.parameters)
  File "/usr/local/lib/python3.7/site-packages/airflow/hooks/dbapi_hook.py", line 159, in run
    with closing(self.get_conn()) as conn:
  File "/usr/local/lib/python3.7/site-packages/airflow/hooks/postgres_hook.py", line 75, in get_conn
    self.conn = psycopg2.connect(**conn_args)
  File "/usr/local/lib/python3.7/site-packages/psycopg2/__init__.py", line 126, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "redshift-cluster-1.c4i0rfzboubh.us-west-2.redshift.amazonaws.com" to address: nodename nor servname provided, or not known

[2019-10-02 19:56:41,952] {taskinstance.py:1074} INFO - Marking task as UP_FOR_RETRY
[2019-10-02 19:56:41,977] {base_task_runner.py:115} INFO - Job 189: Subtask load_final_database_tables Traceback (most recent call last):
[2019-10-02 19:56:41,977] {base_task_runner.py:115} INFO - Job 189: Subtask load_final_database_tables   File "/usr/local/bin/airflow", line 32, in <module>
[2019-10-02 19:56:41,977] {base_task_runner.py:115} INFO - Job 189: Subtask load_final_database_tables     args.func(args)
[2019-10-02 19:56:41,977] {base_task_runner.py:115} INFO - Job 189: Subtask load_final_database_tables   File "/usr/local/lib/python3.7/site-packages/airflow/utils/cli.py", line 74, in wrapper
[2019-10-02 19:56:41,977] {base_task_runner.py:115} INFO - Job 189: Subtask load_final_database_tables     return f(*args, **kwargs)
[2019-10-02 19:56:41,978] {base_task_runner.py:115} INFO - Job 189: Subtask load_final_database_tables   File "/usr/local/lib/python3.7/site-packages/airflow/bin/cli.py", line 522, in run
[2019-10-02 19:56:41,978] {base_task_runner.py:115} INFO - Job 189: Subtask load_final_database_tables     _run(args, dag, ti)
[2019-10-02 19:56:41,978] {base_task_runner.py:115} INFO - Job 189: Subtask load_final_database_tables   File "/usr/local/lib/python3.7/site-packages/airflow/bin/cli.py", line 440, in _run
[2019-10-02 19:56:41,978] {base_task_runner.py:115} INFO - Job 189: Subtask load_final_database_tables     pool=args.pool,
[2019-10-02 19:56:41,978] {base_task_runner.py:115} INFO - Job 189: Subtask load_final_database_tables   File "/usr/local/lib/python3.7/site-packages/airflow/utils/db.py", line 74, in wrapper
[2019-10-02 19:56:41,978] {base_task_runner.py:115} INFO - Job 189: Subtask load_final_database_tables     return func(*args, **kwargs)
[2019-10-02 19:56:41,978] {base_task_runner.py:115} INFO - Job 189: Subtask load_final_database_tables   File "/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 926, in _run_raw_task
[2019-10-02 19:56:41,978] {base_task_runner.py:115} INFO - Job 189: Subtask load_final_database_tables     result = task_copy.execute(context=context)
[2019-10-02 19:56:41,978] {base_task_runner.py:115} INFO - Job 189: Subtask load_final_database_tables   File "/usr/local/lib/python3.7/site-packages/airflow/operators/postgres_operator.py", line 65, in execute
[2019-10-02 19:56:41,978] {base_task_runner.py:115} INFO - Job 189: Subtask load_final_database_tables     self.hook.run(self.sql, self.autocommit, parameters=self.parameters)
[2019-10-02 19:56:41,978] {base_task_runner.py:115} INFO - Job 189: Subtask load_final_database_tables   File "/usr/local/lib/python3.7/site-packages/airflow/hooks/dbapi_hook.py", line 159, in run
[2019-10-02 19:56:41,978] {base_task_runner.py:115} INFO - Job 189: Subtask load_final_database_tables     with closing(self.get_conn()) as conn:
[2019-10-02 19:56:41,978] {base_task_runner.py:115} INFO - Job 189: Subtask load_final_database_tables   File "/usr/local/lib/python3.7/site-packages/airflow/hooks/postgres_hook.py", line 75, in get_conn
[2019-10-02 19:56:41,978] {base_task_runner.py:115} INFO - Job 189: Subtask load_final_database_tables     self.conn = psycopg2.connect(**conn_args)
[2019-10-02 19:56:41,978] {base_task_runner.py:115} INFO - Job 189: Subtask load_final_database_tables   File "/usr/local/lib/python3.7/site-packages/psycopg2/__init__.py", line 126, in connect
[2019-10-02 19:56:41,978] {base_task_runner.py:115} INFO - Job 189: Subtask load_final_database_tables     conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
[2019-10-02 19:56:41,978] {base_task_runner.py:115} INFO - Job 189: Subtask load_final_database_tables psycopg2.OperationalError: could not translate host name "redshift-cluster-1.c4i0rfzboubh.us-west-2.redshift.amazonaws.com" to address: nodename nor servname provided, or not known
[2019-10-02 19:56:41,979] {base_task_runner.py:115} INFO - Job 189: Subtask load_final_database_tables 
[2019-10-02 19:56:45,476] {logging_mixin.py:95} INFO - [[34m2019-10-02 19:56:45,476[0m] {[34mlocal_task_job.py:[0m105} INFO[0m - Task exited with return code 1[0m
